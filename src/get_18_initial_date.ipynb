{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on books category first; We want to limit the data range to 2 years period; To do so, we want to filter by first review dates of the products in the books category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leahtan/Documents/3_Research/2024-Ali/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import get_initial_dates\n",
    "importlib.reload(get_initial_dates)\n",
    "from get_initial_dates import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1: {\"overall\": 5.0, \"verified\": false, \"reviewTime\": \"08 12, 2005\", \"reviewerID\": \"A1C6M8LCIX4M6M\", \"asin\": \"0001713353\", \"style\": {\"Format:\": \" Paperback\"}, \"reviewerName\": \"June Bug\", \"reviewText\": \"This book is a winner with both of my boys.  They really enjoy the pictures and the story.  It's a classic.\", \"summary\": \"Children's favorite\", \"unixReviewTime\": 1123804800}\n",
      "Line 2: {\"overall\": 5.0, \"verified\": false, \"reviewTime\": \"03 30, 2005\", \"reviewerID\": \"A1REUF3A1YCPHM\", \"asin\": \"0001713353\", \"style\": {\"Format:\": \" Hardcover\"}, \"reviewerName\": \"TW Ervin II\", \"reviewText\": \"The King, the Mice and the Cheese by Nancy Gurney is an excellent children's book.  It is one that I well remember from my own childhood and purchased for my daughter who loves it.\\n\\nIt is about a king who has trouble with rude mice eating his cheese. He consults his wise men and they suggest cats to chase away the mice. The cats become a nuisance, so the wise men recommend the king bring in dogs to chase the cats away.  The cycle goes on until the mice are finally brought back to chase away the elephants, brought in to chase away the lions that'd chased away the dogs.\\n\\nThe story ends in compromise and friendship between the mice and the king.  The story also teaches cause and effect relationships.\\n\\nThe pictures that accompany the story are humorous and memorable.  I was thrilled to discover that it is back in print.  I *highly* recommend it for children ages 2 to 7.\", \"summary\": \"A story children will love and learn from\", \"unixReviewTime\": 1112140800}\n",
      "Line 3: {\"overall\": 5.0, \"vote\": \"5\", \"verified\": false, \"reviewTime\": \"04 4, 2004\", \"reviewerID\": \"A1YRBRK2XM5D5\", \"asin\": \"0001713353\", \"style\": {\"Format:\": \" Hardcover\"}, \"reviewerName\": \"Rebecca L. Menner\", \"reviewText\": \"My daughter got her first copy from her great-grandmother (it had been my father's when he was a child). She loves it so much that she has worn out two copies now. Colorful pictures, easy to follow words, and wonderful morals (sharing, thinking problems through, and that the easy way isn't always the best way).\", \"summary\": \"Third copy\", \"unixReviewTime\": 1081036800}\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/'\n",
    "category = 'Books'\n",
    "json_file = data_path + category + '.json'\n",
    "\n",
    "# Read first 3 lines\n",
    "with open(json_file, 'r') as f:\n",
    "    for i in range(3):\n",
    "        print(f'Line {i+1}: {f.readline().strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reviewTime  timestamp\n",
      "0  08 12, 2005 2005-08-12\n",
      "1  09 15, 2023 2023-09-15\n",
      "2  12 31, 2022 2022-12-31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test data\n",
    "test_data = {\"reviewTime\": [\"08 12, 2005\", \"09 15, 2023\", \"12 31, 2022\"]}\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# Convert using the correct format\n",
    "df_test['timestamp'] = pd.to_datetime(df_test['reviewTime'])\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Records Processed: 51,310,000\n",
      "Unique Products: 2,929,846\n",
      "Memory Usage: 1286.3 MB\n",
      "Time Elapsed: 2301.8 seconds\n",
      "Processing Rate: 22291.5 records/second\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "#get the initial dates\n",
    "category = 'Books'\n",
    "data_path = '../../data/'\n",
    "category = 'Books'\n",
    "json_file = data_path + category + '.json'\n",
    "first_review_dates = get_first_review_dates_local(json_file, category=category, timestamp_col='reviewTime', version=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test output\n",
    "path = '../../data/Books/first_review_dates.parquet'\n",
    "first_review_date = pd.read_parquet(path)\n",
    "#distribution of first review dates on year basis\n",
    "first_review_date['year'] = first_review_date['first_review_date'].dt.year\n",
    "print(first_review_date['year'].value_counts().sort_index())\n",
    "del first_review_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Records Processed: 51,300,000\n",
      "Filtered Records: 76,571\n",
      "Memory Usage: 1285.8 MB\n",
      "Processing Rate: 19802.3 records/second\n",
      "Time Elapsed: 2590.6 seconds\n",
      "========================================\n",
      "Final chunks saved with 3155 records\n"
     ]
    }
   ],
   "source": [
    "#now, filter based on dates; get reviews from 2016 onwards\n",
    "path = '../../data/Books/first_review_dates.parquet'\n",
    "data_path = '../../data/'\n",
    "category = 'Books'\n",
    "json_file = data_path + category + '.json'\n",
    "df_first_reviews = pd.read_parquet(path)\n",
    "recent_asins = set(df_first_reviews[df_first_reviews['first_review_date'] > '2016-01-01'].index)\n",
    "filtered_data = process_filtered_dataset_local(file_path = json_file, category=category, asins_to_keep=recent_asins, timestamp_col='reviewTime', version=2018) #batch_size=10000, saved with chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Records Processed: 29,400,000\n",
      "Filtered Records: 8,644\n",
      "Memory Usage: 1396.8 MB\n",
      "Processing Rate: 6374.8 records/second\n",
      "Time Elapsed: 4611.9 seconds\n",
      "========================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'category_23' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#next, get 2023 data of the same asin list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## filter asins using the same var: recent_asins\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Usage with optimizations\u001b[39;00m\n\u001b[1;32m      4\u001b[0m category \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBooks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_filtered_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masins_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecent_asins\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#2023 data;\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3_Research/2024-Ali/Amazon_US/src/get_initial_dates.py:122\u001b[0m, in \u001b[0;36mprocess_filtered_dataset\u001b[0;34m(batch_size, category, asins_to_keep)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processed_chunks:\n\u001b[1;32m    120\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_23\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m     pd\u001b[38;5;241m.\u001b[39mconcat(processed_chunks)\u001b[38;5;241m.\u001b[39mto_parquet(\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_23\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/filtered_data_remaining.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal chunks saved with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunk)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mchunk\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mprocessed_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'category_23' is not defined"
     ]
    }
   ],
   "source": [
    "#next, get 2023 data of the same asin list\n",
    "## filter asins using the same var: recent_asins\n",
    "# Usage with optimizations\n",
    "category = \"Books\"\n",
    "filtered_data = process_filtered_dataset(category=category, asins_to_keep=recent_asins) #2023 data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>A2R9E427N3BOV2</td>\n",
       "      <td>0001939777</td>\n",
       "      <td>None</td>\n",
       "      <td>Theresa M.</td>\n",
       "      <td>Everything was greatthe story, the lion (Aslan...</td>\n",
       "      <td>We want six stars!</td>\n",
       "      <td>1523664000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8022</th>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-04-14</td>\n",
       "      <td>A31HTC55E2C7Z</td>\n",
       "      <td>0001939777</td>\n",
       "      <td>None</td>\n",
       "      <td>Ryan O</td>\n",
       "      <td>This is an 8.5x11 (magazine size) paperback co...</td>\n",
       "      <td>Not the C.S. Lewis version</td>\n",
       "      <td>1523664000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>AL66F5NF82A7N</td>\n",
       "      <td>0001939777</td>\n",
       "      <td>None</td>\n",
       "      <td>BAKER</td>\n",
       "      <td>A classic. I will read this to my kid/grandkid...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1523232000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8024</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>A962F1LM3YN21</td>\n",
       "      <td>0001939777</td>\n",
       "      <td>None</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Still a great book!  I cant wait to read the s...</td>\n",
       "      <td>Classic</td>\n",
       "      <td>1523145600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8025</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-04-03</td>\n",
       "      <td>ABR2ZKZ1M1HRM</td>\n",
       "      <td>0001939777</td>\n",
       "      <td>None</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>I wanted to introduce my little cousins to som...</td>\n",
       "      <td>... to introduce my little cousins to some of ...</td>\n",
       "      <td>1522713600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall  verified reviewTime      reviewerID        asin style  \\\n",
       "8021      5.0      True 2018-04-14  A2R9E427N3BOV2  0001939777  None   \n",
       "8022      2.0      True 2018-04-14   A31HTC55E2C7Z  0001939777  None   \n",
       "8023      5.0     False 2018-04-09   AL66F5NF82A7N  0001939777  None   \n",
       "8024      5.0      True 2018-04-08   A962F1LM3YN21  0001939777  None   \n",
       "8025      5.0      True 2018-04-03   ABR2ZKZ1M1HRM  0001939777  None   \n",
       "\n",
       "         reviewerName                                         reviewText  \\\n",
       "8021       Theresa M.  Everything was greatthe story, the lion (Aslan...   \n",
       "8022           Ryan O  This is an 8.5x11 (magazine size) paperback co...   \n",
       "8023            BAKER  A classic. I will read this to my kid/grandkid...   \n",
       "8024  Amazon Customer  Still a great book!  I cant wait to read the s...   \n",
       "8025  Amazon Customer  I wanted to introduce my little cousins to som...   \n",
       "\n",
       "                                                summary  unixReviewTime  vote  \\\n",
       "8021                                 We want six stars!      1523664000  None   \n",
       "8022                         Not the C.S. Lewis version      1523664000  None   \n",
       "8023                                         Five Stars      1523232000  None   \n",
       "8024                                            Classic      1523145600  None   \n",
       "8025  ... to introduce my little cousins to some of ...      1522713600  None   \n",
       "\n",
       "     image  \n",
       "8021  None  \n",
       "8022  None  \n",
       "8023  None  \n",
       "8024  None  \n",
       "8025  None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.0</td>\n",
       "      <td>yep. need this.</td>\n",
       "      <td>yep.  need this.</td>\n",
       "      <td>[]</td>\n",
       "      <td>0718032152</td>\n",
       "      <td>0718032152</td>\n",
       "      <td>AGCI7FAH4GL5FI65HYLKWTMFZ2CQ</td>\n",
       "      <td>2017-02-22 17:49:51.000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>I love it.</td>\n",
       "      <td>[]</td>\n",
       "      <td>0931722470</td>\n",
       "      <td>0931722470</td>\n",
       "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
       "      <td>2017-09-08 00:21:28.659</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love it.</td>\n",
       "      <td>[]</td>\n",
       "      <td>0931722306</td>\n",
       "      <td>0931722306</td>\n",
       "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
       "      <td>2017-09-08 00:21:15.806</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.0</td>\n",
       "      <td>A wonderful start to an interesting world</td>\n",
       "      <td>This is a great start to a world filled with r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1717827136</td>\n",
       "      <td>1717827136</td>\n",
       "      <td>AFTC6ZR5IKNRDG5JCPVNVMU3XV2Q</td>\n",
       "      <td>2018-07-26 20:21:31.287</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>As a woman in this industry...</td>\n",
       "      <td>I laughed and cried. Sad that the industry has...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1501110624</td>\n",
       "      <td>1501110624</td>\n",
       "      <td>AGBUJDDLRJIUJFTKPABJT6CJTHRQ</td>\n",
       "      <td>2016-02-26 00:16:29.000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rating                                      title  \\\n",
       "27     5.0                            yep. need this.   \n",
       "34     5.0                                 Five Stars   \n",
       "35     5.0                                 Five Stars   \n",
       "37     5.0  A wonderful start to an interesting world   \n",
       "48     5.0             As a woman in this industry...   \n",
       "\n",
       "                                                 text images        asin  \\\n",
       "27                                   yep.  need this.     []  0718032152   \n",
       "34                                         I love it.     []  0931722470   \n",
       "35                                           Love it.     []  0931722306   \n",
       "37  This is a great start to a world filled with r...     []  1717827136   \n",
       "48  I laughed and cried. Sad that the industry has...     []  1501110624   \n",
       "\n",
       "   parent_asin                       user_id               timestamp  \\\n",
       "27  0718032152  AGCI7FAH4GL5FI65HYLKWTMFZ2CQ 2017-02-22 17:49:51.000   \n",
       "34  0931722470  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2017-09-08 00:21:28.659   \n",
       "35  0931722306  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2017-09-08 00:21:15.806   \n",
       "37  1717827136  AFTC6ZR5IKNRDG5JCPVNVMU3XV2Q 2018-07-26 20:21:31.287   \n",
       "48  1501110624  AGBUJDDLRJIUJFTKPABJT6CJTHRQ 2016-02-26 00:16:29.000   \n",
       "\n",
       "    helpful_vote  verified_purchase  \n",
       "27             0              False  \n",
       "34             0               True  \n",
       "35             0               True  \n",
       "37             1               True  \n",
       "48             0               True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path1 = '/Users/leahtan/Documents/3_Research/2024-Ali/data/Books/filtered_data_100000.parquet'\n",
    "path2 = '/Users/leahtan/Documents/3_Research/2024-Ali/data/Books_23/filtered_data_100000.parquet'\n",
    "display(pd.read_parquet(path1).head())\n",
    "display(pd.read_parquet(path2).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#may need to add the last batch result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_records_in_a(folder_a, folder_b):\n",
    "    start_time = time.time()\n",
    "    files_a = list(Path(folder_a).glob('filtered_data_*.parquet'))\n",
    "    files_b = list(Path(folder_b).glob('filtered_data_*.parquet'))\n",
    "    \n",
    "    key_columns_2018 = ['reviewTime', 'reviewerID', 'asin','reviewText']\n",
    "    key_columns_2023 = ['timestamp', 'user_id', 'asin','text'] \n",
    "    \n",
    "    # Load or create 2023 keys\n",
    "    save_path = os.path.join(folder_b, 'keys_2023.parquet')\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"Loading 2023 keys...\")\n",
    "        keys_2023_df = pd.read_parquet(save_path)\n",
    "        keys_2023_basic = set(zip(keys_2023_df['Timekey'], keys_2023_df['user_id'], keys_2023_df['asin']))\n",
    "        keys_2023_text = set(zip(keys_2023_df['Timekey'], keys_2023_df['asin'], keys_2023_df['text']))\n",
    "    else:\n",
    "        print(\"Building 2023 keys set...\")\n",
    "        keys_all = set()\n",
    "        for file_b in files_b:\n",
    "            chunk_b = pd.read_parquet(file_b, columns=key_columns_2023)\n",
    "            chunk_b['Timekey'] = pd.to_datetime(chunk_b['timestamp'], unit='s').dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Create both types of keys\n",
    "            keys_all.update(zip(chunk_b['Timekey'], chunk_b['user_id'], chunk_b['asin'], chunk_b['text']))\n",
    "        \n",
    "        keys_2023_df = pd.DataFrame(keys_all, columns=['Timekey', 'user_id', 'asin', 'text'])\n",
    "        keys_2023_basic = set(zip(keys_2023_df['Timekey'], keys_2023_df['user_id'], keys_2023_df['asin']))\n",
    "        keys_2023_text = set(zip(keys_2023_df['Timekey'], keys_2023_df['asin'], keys_2023_df['text']))\n",
    "        keys_2023_df.to_parquet(save_path)\n",
    "        print(\"Keys saved.\")\n",
    "        del keys_2023_df\n",
    "        del keys_all\n",
    "    \n",
    "    # Load or create 2018 keys\n",
    "    save_path = os.path.join(folder_a, 'keys_2018.parquet')\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"Loading 2018 keys...\")\n",
    "        keys_2018_df = pd.read_parquet(save_path)\n",
    "        keys_2018_basic = set(zip(keys_2018_df['Timekey'], keys_2018_df['reviewerID'], keys_2018_df['asin']))\n",
    "        keys_2018_text = set(zip(keys_2018_df['Timekey'], keys_2018_df['asin'], keys_2018_df['reviewText']))\n",
    "    else:\n",
    "        print(\"Building 2018 keys set...\")\n",
    "        keys_2018_all = set()\n",
    "\n",
    "        for file_a in files_a:\n",
    "            chunk_a = pd.read_parquet(file_a, columns=key_columns_2018)\n",
    "            chunk_a['Timekey'] = pd.to_datetime(chunk_a['reviewTime']).dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            keys_2018_all.update(zip(chunk_a['Timekey'], chunk_a['reviewerID'], chunk_a['asin'], chunk_a['reviewText']))\n",
    "            #form basic and text keys\n",
    "    \n",
    "        keys_2018_df = pd.DataFrame(keys_2018_all, columns=['Timekey', 'reviewerID', 'asin', 'reviewText'])\n",
    "        keys_2018_basic = set(zip(keys_2018_df['Timekey'], keys_2018_df['reviewerID'], keys_2018_df['asin']))\n",
    "        keys_2018_text = set(zip(keys_2018_df['Timekey'], keys_2018_df['asin'], keys_2018_df['reviewText']))\n",
    "        keys_2018_df.to_parquet(save_path)\n",
    "        print(\"Keys saved.\")\n",
    "        del keys_2018_df\n",
    "        del keys_2018_all\n",
    "\n",
    "    # Compare keys\n",
    "    total18 = len(keys_2018_basic)\n",
    "    print(\"\\nComparing keys...\")\n",
    "    print(f\"1) Basic key comparison (Timekey, ID, ASIN):\")\n",
    "    unique_basic = keys_2018_basic - keys_2023_basic\n",
    "    print(f\"Records unique to 2018: {len(unique_basic):,}\")\n",
    "    print(f\"Proportion unique to 2018: {len(unique_basic)/total18:.2%}\")\n",
    "    \n",
    "    print(f\"\\n2) Text comparison:\")\n",
    "    unique_text = keys_2018_text - keys_2023_text\n",
    "    print(f\"Reviews with unique text: {len(unique_text):,}\")\n",
    "    print(f\"Proportion unique text: {len(unique_text)/total18:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'unique_basic_keys': unique_basic,\n",
    "        'unique_text': unique_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 2023 keys set...\n",
      "Keys saved.\n",
      "Building 2018 keys set...\n",
      "Keys saved.\n",
      "\n",
      "Comparing keys...\n",
      "1) Basic key comparison (Timekey, ID, ASIN):\n",
      "Records unique to 2018: 7,764,565\n",
      "Proportion unique to 2018: 100.00%\n",
      "\n",
      "2) Text comparison:\n",
      "Reviews with unique text: 6,912,054\n",
      "Proportion unique text: 89.02%\n"
     ]
    }
   ],
   "source": [
    "folder_a = '/Users/leahtan/Documents/3_Research/2024-Ali/data/Books'\n",
    "folder_b = '/Users/leahtan/Documents/3_Research/2024-Ali/data/Books_23'\n",
    "unique_records_18 = find_unique_records_in_a(folder_a, folder_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common asins: 451,523\n",
      "Unique asins in 2018: 168,003\n",
      "Unique asins in 2023: 0\n",
      "proportion of common asins: 72.88%\n"
     ]
    }
   ],
   "source": [
    "#we should focus on asins which are in both 2018 and 2023 data; \n",
    "# some products may be deleted completely in 2023;\n",
    "#check asins in both 2018 and 2023 data\n",
    "keys_2018_df = pd.read_parquet('/Users/leahtan/Documents/3_Research/2024-Ali/data/Books/keys_2018.parquet', columns=['asin'])\n",
    "keys_2023_df = pd.read_parquet('/Users/leahtan/Documents/3_Research/2024-Ali/data/Books_23/keys_2023.parquet', columns=['asin'])\n",
    "asins_2018 = set(keys_2018_df['asin'])\n",
    "asins_2023 = set(keys_2023_df['asin'])\n",
    "common_asins = asins_2018.intersection(asins_2023)\n",
    "print(f\"Common asins: {len(common_asins):,}\")\n",
    "print(f\"Unique asins in 2018: {len(asins_2018 - asins_2023):,}\")\n",
    "print(f\"Unique asins in 2023: {len(asins_2023 - asins_2018):,}\")\n",
    "print(f'proportion of common asins: {len(common_asins)/len(asins_2018):.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
